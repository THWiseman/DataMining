{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "def purity_score(y_true, y_pred, axis):\n",
        "    # compute contingency matrix (also called confusion matrix)\n",
        "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
        "    # return purity\n",
        "    return np.sum(np.amax(contingency_matrix, axis=axis)) / np.sum(contingency_matrix) \n",
        "\n",
        "\n",
        "def evaluate_k_means(kmeans, name, data, labels):\n",
        "  estimator = make_pipeline(StandardScaler(), kmeans).fit(data)\n",
        "  results = [name, estimator[-1].inertia_]\n",
        "  score = metrics.homogeneity_score(labels, estimator[-1].labels_)\n",
        "  purityX = purity_score(labels, estimator[-1].labels_, 0)\n",
        "  purityY = purity_score(labels,estimator[-1].labels_, 1)\n",
        "\n",
        "  print(\"Name: \", results[0])\n",
        "  print(\"Objective (i.e. inertia): \", results[1])\n",
        "  print(\"Gini Score: \", score)\n",
        "  print(\"Row Purity: \", purityX)\n",
        "  print(\"Column Purity: \", purityY)\n",
        "  print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "swgcGI_3hR2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "TAFcUfG65lDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#MNIST\n",
        "dataPath = \"sample_data/mnist_train_small.csv\"\n",
        "dataFile = open(dataPath, 'r')\n",
        "trainDF = pd.read_csv(dataPath)\n",
        "trainFeatures = trainDF.iloc[:,1:]\n",
        "trainLabels = trainDF.iloc[:,:1].values.flatten().astype(np.int32)\n",
        "\n",
        "kmeans = KMeans(n_clusters=5,random_state=0).fit(trainFeatures)\n",
        "evaluate_k_means(kmeans, \"Mnist K = 5\", trainFeatures, trainLabels)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10,random_state=0).fit(trainFeatures)\n",
        "evaluate_k_means(kmeans, \"Mnist K = 10\", trainFeatures, trainLabels)\n",
        "\n",
        "kmeans = KMeans(n_clusters=20,random_state=0).fit(trainFeatures)\n",
        "evaluate_k_means(kmeans, \"Mnist K = 20\", trainFeatures, trainLabels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGB2Cp7XrCUc",
        "outputId": "52968dbf-86b0-49ac-dcf9-235a6e109c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name:  Mnist K = 5\n",
            "Objective (i.e. inertia):  12434543.486521289\n",
            "Gini Score:  0.3297481445651872\n",
            "Row Purity:  0.41012050602530126\n",
            "Column Purity:  0.7098854942747137\n",
            "\n",
            "\n",
            "Name:  Mnist K = 10\n",
            "Objective (i.e. inertia):  11613063.544049602\n",
            "Gini Score:  0.4233432910668482\n",
            "Row Purity:  0.5288764438221911\n",
            "Column Purity:  0.5720286014300715\n",
            "\n",
            "\n",
            "Name:  Mnist K = 20\n",
            "Objective (i.e. inertia):  10756085.039449332\n",
            "Gini Score:  0.5125123104171879\n",
            "Row Purity:  0.6093804690234512\n",
            "Column Purity:  0.40852042602130106\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Fashion\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "dataPath = \"/content/drive/MyDrive/DataMining/Fashion/fashion-mnist_train.csv\"\n",
        "dataFile = open(dataPath, 'r')\n",
        "\n",
        "#load the data\n",
        "trainDF = pd.read_csv(dataPath)\n",
        "\n",
        "labels = trainDF.iloc[:,:1].values.flatten()\n",
        "features = trainDF.iloc[:,1:]\n",
        "\n",
        "kmeans = KMeans(n_clusters=5,random_state=0).fit(features)\n",
        "evaluate_k_means(kmeans, \"Fashion K = 5\", features, labels)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10,random_state=0).fit(features)\n",
        "evaluate_k_means(kmeans, \"Fashion K = 10\", features, labels)\n",
        "\n",
        "kmeans = KMeans(n_clusters=20,random_state=0).fit(features)\n",
        "evaluate_k_means(kmeans, \"Fashion K = 20\", features, labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK3p2gDf3Ns5",
        "outputId": "4f0916f9-5961-46ea-a00f-3d901102ec0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name:  Fashion K = 5\n",
            "Objective (i.e. inertia):  31124733.130651943\n",
            "Gini Score:  0.3642661390898533\n",
            "Row Purity:  0.3777\n",
            "Column Purity:  0.69165\n",
            "\n",
            "\n",
            "Name:  Fashion K = 10\n",
            "Objective (i.e. inertia):  26154004.41527849\n",
            "Gini Score:  0.4948062577384306\n",
            "Row Purity:  0.5547166666666666\n",
            "Column Purity:  0.5968333333333333\n",
            "\n",
            "\n",
            "Name:  Fashion K = 20\n",
            "Objective (i.e. inertia):  22488430.322118796\n",
            "Gini Score:  0.6066827784650357\n",
            "Row Purity:  0.6613166666666667\n",
            "Column Purity:  0.45025\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "dataset = fetch_20newsgroups(\n",
        "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
        "    subset=\"all\",\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "labels = dataset.target\n",
        "unique_labels, category_sizes = np.unique(labels, return_counts=True)\n",
        "true_k = unique_labels.shape[0]\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_df=0.5,\n",
        "    min_df=5,\n",
        "    stop_words=\"english\",\n",
        ")\n",
        "X_tfidf = vectorizer.fit_transform(dataset.data)\n",
        "\n",
        "\n",
        "def evaluate_k_means(kmeans, name, data, labels):\n",
        "  estimator = make_pipeline(StandardScaler(with_mean=False), kmeans).fit(data)\n",
        "  results = [name, estimator[-1].inertia_]\n",
        "  score = metrics.homogeneity_score(labels, estimator[-1].labels_)\n",
        "  purityX = purity_score(labels, estimator[-1].labels_, 0)\n",
        "  purityY = purity_score(labels,estimator[-1].labels_, 1)\n",
        "\n",
        "  print(\"Name: \", results[0])\n",
        "  print(\"Objective (i.e. inertia): \", results[1])\n",
        "  print(\"Gini Score: \", score)\n",
        "  print(\"Row Purity: \", purityX)\n",
        "  print(\"Column Purity: \", purityY)\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "kmeans = KMeans(\n",
        "        n_clusters=20,\n",
        "        random_state=0,\n",
        "        ).fit(X_tfidf)\n",
        "evaluate_k_means(kmeans, \"20NG K = 20\", X_tfidf, labels)\n",
        "\n",
        "kmeans = KMeans(\n",
        "        n_clusters=40,\n",
        "        random_state=0,\n",
        "        ).fit(X_tfidf)\n",
        "evaluate_k_means(kmeans, \"20NG K = 40\", X_tfidf, labels)\n",
        "\n",
        "kmeans = KMeans(\n",
        "        n_clusters=10,\n",
        "        random_state=0,\n",
        "        ).fit(X_tfidf)\n",
        "evaluate_k_means(kmeans, \"20NG K = 10\", X_tfidf, labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAFwfelF4VpB",
        "outputId": "5d63c0de-ce4c-4ef0-f2ad-d2813b5eb9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name:  20NG K = 20\n",
            "Objective (i.e. inertia):  441939139.3143358\n",
            "Gini Score:  0.11939048079216033\n",
            "Row Purity:  0.10267430754536772\n",
            "Column Purity:  0.8877215324206729\n",
            "\n",
            "\n",
            "Name:  20NG K = 40\n",
            "Objective (i.e. inertia):  436141449.87681043\n",
            "Gini Score:  0.004720371855807601\n",
            "Row Purity:  0.057624960203756764\n",
            "Column Purity:  0.9952244508118434\n",
            "\n",
            "\n",
            "Name:  20NG K = 10\n",
            "Objective (i.e. inertia):  445831593.4926965\n",
            "Gini Score:  0.002183698799859606\n",
            "Row Purity:  0.05518412395203226\n",
            "Column Purity:  0.9975061020906293\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "class GMM:\n",
        "  def __init__(self, k, max_iter=5):\n",
        "    self.k = k\n",
        "    self.max_iter = int(max_iter)\n",
        "\n",
        "  def initialize(self, X):\n",
        "    self.shape = X.shape #store the shape of the given data matrix. N points each with M features.\n",
        "    self.n, self.m = self.shape\n",
        "\n",
        "    self.phi = np.full(shape=self.k, fill_value=1/self.k) #the probability of each class. \n",
        "    self.weights = np.full( shape=self.shape, fill_value=1/self.k)  #the likelihood that a point belongs to cluster k. initially, all clusters are equally likely. \n",
        "    \n",
        "    random_row = np.random.randint(low=0, high=self.n, size=self.k) \n",
        "    self.mu = [  X[row_index,:] for row_index in random_row ] #set the mean of each cluster to be the value of a random point in the dataset. \n",
        "    self.sigma = [ np.cov(X.T) for _ in range(self.k) ] #set the sigma of each cluster to be the covariance of that row\n",
        "\n",
        "  def e_step(self, X):\n",
        "    #Given a mu and sigma, update the liklihood that point i came from cluster k.\n",
        "    self.weights = self.predict_proba(X) #each row is a datapoint, each column is the likelihood that that datapoint is in cluster k\n",
        "    #the overall probability of a random point coming from cluster k is the average of all of the points coming from cluster k. \n",
        "    self.phi = self.weights.mean(axis=0)\n",
        "\n",
        "  def m_step(self, X):\n",
        "    #Given the likehiood that point i came from cluster k, determine new mu and sigma for each cluster k.\n",
        "    for i in range(self.k):\n",
        "      weight = self.weights[:,[i]] #th\n",
        "      total_weight = weight.sum()\n",
        "      #the mean of cluster i is the average of all points in that cluster.\n",
        "      #Each \"point\" is actually a fraction of a point determined by its weight to be in that cluster. \n",
        "      #So, to get the number of \"points\" in the cluster, we multiply every n by its weight and then sum them up.\n",
        "      #then, we divide by the total_weight to average them. \n",
        "      self.mu[i] = (X * weight).sum(axis=0) / total_weight\n",
        "      self.sigma[i] = np.cov(X.T, aweights=(weight/total_weight).flatten(), bias=True)\n",
        "\n",
        "  def fit(self, X):\n",
        "    self.initialize(X)\n",
        "    for iteration in range(self.max_iter):\n",
        "      self.e_step(X)\n",
        "      self.m_step(X)\n",
        "  \n",
        "  def predict_proba(self, X):\n",
        "    likelihood = np.zeros((self.n, self.k))\n",
        "    for i in range(self.k): #for each cluster\n",
        "      distribution = multivariate_normal( #make a distribution for that cluster given our current mu and sigma values \n",
        "          mean=self.mu[i],\n",
        "          cov=self.sigma[i])\n",
        "      likelihood[:,i] = distribution.pdf(X) #each i,j in likelihood is the likelihood that point i is in cluster j. \n",
        "    \n",
        "    numerator = likelihood * self.phi #bayes\n",
        "    denominator = numerator.sum(axis=1)[:,np.newaxis]\n",
        "    weights = numerator / denominator\n",
        "    return weights\n",
        "\n",
        "  def predict(self, X):\n",
        "    weights = self.predict_proba(X) #each row is a datapoint, each column is the likelihood that that datapoint is in cluster k\n",
        "    return np.argmax(weights,axis=1) #to get the class index of a point, find the class index it is most likely to be in and return that.\n",
        "\n",
        "  \n",
        "  def print(self):\n",
        "    print(\"GMM Model with k=\", self.k, \" and data matrix with shape \", self.shape, \":\\n\")\n",
        "    for i in range(self.k):\n",
        "      print(\"Mean for cluster \", i, \":\\n \", self.mu[i])\n",
        "      print(\"Cov for cluster \", i, \":\\n \", self.sigma[i])\n",
        "      print(\"Number of points in cluster \", i ,\":\\n \", np.sum(self.weights[:,i]), \"\\n\")\n",
        "      print(\"\\n\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zxkOxZLPENTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from scipy.stats import mode\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "path2 = \"/content/drive/MyDrive/DataMining/2gaussian.txt\"\n",
        "path3 = \"/content/drive/MyDrive/DataMining/3gaussian.txt\"\n",
        "\n",
        "df = pd.read_table(path2, delimiter = ' ')\n",
        "\n",
        "X = df.values\n",
        "print(X.shape)\n",
        "\n",
        "gmm = GMM(k=2, max_iter=1000)\n",
        "gmm.fit(X)\n",
        "gmm.print()\n",
        "\n",
        "\n",
        "df = pd.read_table(path3, delimiter = ' ')\n",
        "\n",
        "X = df.values\n",
        "\n",
        "gmm2 = GMM(k=3, max_iter=1000)\n",
        "gmm2.fit(X)\n",
        "gmm2.print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeugL129VADf",
        "outputId": "dc1de8d6-faaf-4f20-c39f-4b9055f05d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5999, 2)\n",
            "GMM Model with k= 2  and data matrix with shape  (5999, 2) :\n",
            "\n",
            "Mean for cluster  0 :\n",
            "  [7.01295317 3.98321625]\n",
            "Cov for cluster  0 :\n",
            "  [[0.97503308 0.4977206 ]\n",
            " [0.4977206  1.00138972]]\n",
            "Number of points in cluster  0 :\n",
            "  3990.3243929123364 \n",
            "\n",
            "\n",
            "\n",
            "Mean for cluster  1 :\n",
            "  [2.99404367 3.05211315]\n",
            "Cov for cluster  1 :\n",
            "  [[1.01010905 0.02721332]\n",
            " [0.02721332 2.93789732]]\n",
            "Number of points in cluster  1 :\n",
            "  2008.6756070876638 \n",
            "\n",
            "\n",
            "\n",
            "GMM Model with k= 3  and data matrix with shape  (9999, 2) :\n",
            "\n",
            "Mean for cluster  0 :\n",
            "  [7.02158184 4.01547592]\n",
            "Cov for cluster  0 :\n",
            "  [[0.99039767 0.50094769]\n",
            " [0.50094769 0.9956332 ]]\n",
            "Number of points in cluster  0 :\n",
            "  2984.324455881726 \n",
            "\n",
            "\n",
            "\n",
            "Mean for cluster  1 :\n",
            "  [5.01170274 7.00144464]\n",
            "Cov for cluster  1 :\n",
            "  [[0.97975418 0.18521522]\n",
            " [0.18521522 0.97458008]]\n",
            "Number of points in cluster  1 :\n",
            "  4959.684040771087 \n",
            "\n",
            "\n",
            "\n",
            "Mean for cluster  2 :\n",
            "  [3.03982973 3.04846728]\n",
            "Cov for cluster  2 :\n",
            "  [[1.02920341 0.02697752]\n",
            " [0.02697752 3.38659663]]\n",
            "Number of points in cluster  2 :\n",
            "  2054.991503347187 \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 4\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture as gm\n",
        "from sklearn import metrics as metrics\n",
        "\n",
        "#Load the fashion dataset\n",
        "dataPath = \"/content/drive/MyDrive/DataMining/Fashion/fashion-mnist_train.csv\"\n",
        "df = pd.read_csv(dataPath)\n",
        "\n",
        "#Get labels and features\n",
        "labels = df.iloc[:,:1].values.flatten()\n",
        "features = df.iloc[:,1:]\n",
        "features=(features-features.mean())/features.std()\n",
        "\n",
        "print(\"Labaels shape: \", labels.shape)\n",
        "print(\"Features shape: \", features.shape)\n",
        "X = features.values\n",
        "\n",
        "\n",
        "gmm = gm(n_components = 10, init_params='kmeans',\n",
        "           n_init = 5, max_iter = 5000, covariance_type = 'diag')\n",
        "\n",
        "gmm.fit(X)\n",
        "\n",
        "predictions = gmm.predict(X)\n",
        "\n",
        "print (\"gmm: silhouttte: \", metrics.silhouette_score(X, predictions))"
      ],
      "metadata": {
        "id": "mm-HMaN8b7FD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9adef1db-1fca-464b-9db1-2434a0956962"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labaels shape:  (60000,)\n",
            "Features shape:  (60000, 784)\n",
            "gmm: silhouttte:  0.08464173957612584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PPy-dsU8X8mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from scipy.stats import mode\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.preprocessing as preprocessing\n",
        "\n",
        "path = \"/content/drive/MyDrive/DataMining/spambase.data\"\n",
        "\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "labels = df.iloc[: , -1]\n",
        "X = df.iloc[:,0:-1].values\n",
        "\n",
        "X = df.values #returns a numpy array\n",
        "\n",
        "\n",
        "gmm = gm(n_components = 2, init_params='kmeans',\n",
        "           n_init = 5, max_iter = 5000, covariance_type = 'diag')\n",
        "\n",
        "gmm.fit(X)\n",
        "predictions = gmm.predict(X)\n",
        "\n",
        "silhouette = metrics.silhouette_score(X, labels)\n",
        "score = metrics.homogeneity_score(predictions, labels)\n",
        "\n",
        "print(\"Spambase Results:\")\n",
        "print(\"Silhouette: \", silhouette)\n",
        "print(\"Homogenity: \", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6LZXOJLORSX",
        "outputId": "209cb127-2635-4658-e717-5e9ff8485b98"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spambase Results:\n",
            "Silhouette:  0.1976264243774234\n",
            "Homogenity:  0.25742583510935213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "_8V9f1yCpYfy"
      }
    }
  ]
}